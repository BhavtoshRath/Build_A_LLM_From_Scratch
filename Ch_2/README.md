---

# Interview Concepts:

- **What is BPE? How is it different from word-pair tokenization**
- Byte Pair Encoding (BPE) is a widely used subword tokenization algorithm in NLP. It is used for handling out-of-vocabulary words and reducing the vocabulary size in large-scale language models.
- Word-level tokenization treats each word as a separate token, which can lead to large vocabularies and fail for rare or unseen words. BPE uses subword units, allowing it to generalize better.