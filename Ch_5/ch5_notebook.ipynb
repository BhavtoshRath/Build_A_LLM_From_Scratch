{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pretraining phaze\n",
    "\n",
    "In this chapter we implement stage 2: the training loop and model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.8.1\n",
      "numpy version: 1.26.4\n",
      "torch version: 2.0.1\n",
      "tensorflow version: 2.16.2\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\",\n",
    "        \"numpy\",\n",
    "        \"torch\",\n",
    "        \"tensorflow\"]\n",
    "\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Evaluating generative text models\n",
    "\n",
    "Initialize GPT model from previous chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# from previous_chapters import MultiHeadAttention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "    \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        \n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        \n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        \n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        \n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        \n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores/ keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        \n",
    "        return context_vec\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"],\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Shorcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        # Shorcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4* cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    \n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 *x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0/ torch.pi)) * (x+0.044715*torch.pow(x,3))))\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 256, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Using GPT to generate text\n",
    "\n",
    "3-step process:use tokenizer to convert text to token IDs, model recieves token IDs and generates logits (prob distribution for each token in the vocab), logits are converted back to token ids.\n",
    "We also use generate_text_simple() from previous chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=-1)  # (batch, n_tokens + 1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define 2 methods: text_to_token_ids(), token_ids_to_text() to convert between token and text representations.  \n",
    "Output is not very good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingFactor incarnationDimCreated beachesPolitics wage Constant conco\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(model=model,\n",
    "                                 idx=text_to_token_ids(start_context, tokenizer),\n",
    "                                 max_new_tokens=10,\n",
    "                                 context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    "                                 )\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Calculate the 'text generation loss': cross-entropy and perplexity\n",
    "\n",
    "In following example, inputs and targets move by 1 position. Lets work with 2-input example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],\n",
    "                       [40, 1107, 588]])\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345],\n",
    "                       [1107, 588, 11311]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we feed inputs into the model to generate logits. Using softmax, we turn logits tensor to probability tensor of same dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "    \n",
    "probas = torch.softmax(logits, dim=-1) # probability of each token in the vocab\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we get position of highest probability score from 50,257 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [45647],\n",
      "         [ 1804]],\n",
      "\n",
      "        [[49906],\n",
      "         [15335],\n",
      "         [20861]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we map output from argmax. Decode the tokens. Output is way off as model not trained yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output batch 1:  Armed sd doing\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Target batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Output batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are token probabilities corresponding to targets. We need to bring them close to 1. Goal of training LLM is to maximize likelihood of correct output token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([9.3956e-05, 2.8011e-05, 1.8640e-05])\n",
      "Text 2: tensor([1.1259e-05, 4.3579e-05, 5.9627e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0,1,2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0,1,2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.2727, -10.4829, -10.8902, -11.3943, -10.0409, -12.0300])\n"
     ]
    }
   ],
   "source": [
    "# We compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.6852)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average probability of each token.\n",
    "# Make this value as large as possible (i.e. as close to 0)\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of maximizing the avg log-probability, we will minimize the negetive log probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.6852)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use cross-entropy implemented by pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape:  torch.Size([2, 3, 50257])\n",
      "Targets shape:  torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape [batch_size, num_tokens, vocab_size]\n",
    "print(\"Logits shape: \", logits.shape)\n",
    "\n",
    "# Targets have shape [batch_size, num_tokens]\n",
    "print(\"Targets shape: \", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For cross-entropy function we want to flatten these tensors by combining them over batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits:  torch.Size([6, 50257])\n",
      "Flattened targets:  torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits: \", logits_flat.shape)\n",
    "print(\"Flattened targets: \", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.6852)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is perplexity? It is the exponential of cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(43702.9180)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the training and validation set losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "print(text_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
     ]
    }
   ],
   "source": [
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters:  20479\n",
      "Tokens:  5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters: \", total_characters)\n",
    "print(\"Tokens: \", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the text into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDataset(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation ratio\n",
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    "    )\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader\")\n",
    "    \n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader: \n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader: \n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader: \")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader: \")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 1st utility function to calculate cross-entropy loss for a given batch\n",
    "2. 2nd utility function computes loss for a user-specified number of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce no. of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss/num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  10.990534146626791\n",
      "Validation loss:  10.977392196655273\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking when no training\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "    \n",
    "print(\"Training loss: \", train_loss)\n",
    "print(\"Validation loss: \", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Training an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we implemented code to calculate training and validation loss.  \n",
    "Now we implement model training code.\n",
    "Some concepts to look intot in the future: lr warmup, gradient clipping, cosine annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen  = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Model set to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # calc. loss gradients\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "            \n",
    "            # Evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, \\\n",
    "                                                      device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss: .3f}\")\n",
    "        \n",
    "        # Print a simple text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "        \n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model,\n",
    "            idx=encoded,\n",
    "            max_new_tokens=50,\n",
    "            context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 10.116, Val loss  10.174\n",
      "Ep 1 (Step 000005): Train loss 8.177, Val loss  8.408\n",
      "Every effort moves you,,,,,,...,,,,,.,,,,,,,,,,,,,,.,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 2 (Step 000010): Train loss 6.950, Val loss  7.135\n",
      "Ep 2 (Step 000015): Train loss 6.275, Val loss  6.713\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 3 (Step 000020): Train loss 6.204, Val loss  6.657\n",
      "Ep 3 (Step 000025): Train loss 6.093, Val loss  6.695\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 4 (Step 000030): Train loss 6.074, Val loss  6.756\n",
      "Ep 4 (Step 000035): Train loss 6.023, Val loss  6.769\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 5 (Step 000040): Train loss 6.033, Val loss  6.846\n",
      "Every effort moves you,....                                             \n",
      "Ep 6 (Step 000045): Train loss 5.989, Val loss  6.845\n",
      "Ep 6 (Step 000050): Train loss 6.044, Val loss  6.808\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,, the,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 7 (Step 000055): Train loss 5.962, Val loss  6.838\n",
      "Ep 7 (Step 000060): Train loss 5.966, Val loss  6.863\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 8 (Step 000065): Train loss 5.946, Val loss  6.829\n",
      "Ep 8 (Step 000070): Train loss 6.033, Val loss  6.802\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 9 (Step 000075): Train loss 5.993, Val loss  6.757\n",
      "Ep 9 (Step 000080): Train loss 5.998, Val loss  6.749\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 10 (Step 000085): Train loss 5.864, Val loss  6.736\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting occurs because we are iterating over a smal training data again and again.\n",
    "It basically memorizes the training data. Later we will see some decoding strategies to prevent this memorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Decoding strategies to control randomness\n",
    "\n",
    "We use generate_text_simple() to generate next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we execute above method multiple times, we will get the same output.\n",
    "Two decoding strategies: temperature scaling and topk sampling\n",
    "\n",
    "#### Temperature scaling:\n",
    "Instead of using torch.argmax() to sample token having highest probability of being next token, we use torch.multinomial(probs, num_samples=1), sampling from a probability distribution.\n",
    "\n",
    "Below is a recap example of how to generate next token..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "vocab = {\n",
    "    \"closer\": 0,\n",
    "    \"every\": 1,\n",
    "    \"effort\": 2,\n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5,\n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "}\n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.9, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically what we did above was that instead of returning result of argmax() we sampled most likely token from a softmax distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 x closer\n",
      "2 x every\n",
      "0 x effort\n",
      "544 x forward\n",
      "2 x inches\n",
      "1 x moves\n",
      "0 x pizza\n",
      "376 x toward\n",
      "4 x you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "        \n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can control the distribution process through process called 'temperature scaling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "temperatures = [1, 0.1, 5]\n",
    "\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.1 has sharper distribution\n",
    "\n",
    "#### topk samplings:\n",
    "We do this to increase output diversity of even higher temperature values and reduce probability of nonsensical tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,    inf,    inf, 6.7500,    inf,    inf,    inf, 6.2800,    inf])\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "\n",
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float('inf')),\n",
    "    other=next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3 Modifying text generation functions\n",
    "\n",
    "Using the above 2 concepts (temperature and topk samplings) to modify the generate_simple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, \n",
    "             temperature=0.0, top_k=None, eos_id=None):\n",
    "    \n",
    "    # For loop is same as before..\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        \n",
    "        # Filter logits with topk sampling\n",
    "        if top_k is not None:\n",
    "            # keep only topk values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, \n",
    "                                 torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "            \n",
    "            if temperature > 0.0:\n",
    "                logits = logits / temperature\n",
    "            \n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (batch_size, 1)\n",
    "            \n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        \n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        \n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you have I a in one to a I it was of to to-- and\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4    \n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Loading and saving model weights in PyTorch\n",
    "\n",
    "Save.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Z004XN6/PycharmProjects/lessr/lessr_venv/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device, weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 'adaptive optimizers' like Adam and AdamW instead of SGD. These optimiers have additonal parameters that can be used later to continue pretrining, s we can save them using code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\"model_state_dict\": model.state_dict(),\n",
    "     \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "     },\n",
    "    \"model_with_optimizer.pth\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_with_optimizer.pth\", weights_only=True)\n",
    "\n",
    "model=GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Loading pretrained weights from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.16.2\n",
      "tqdm version:  4.62.3\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version: \", version(\"tensorflow\"))\n",
    "print(\"tqdm version: \", version(\"tqdm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 13:09:52.219195: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/Z004XN6/PycharmProjects/lessr/lessr_venv/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|███████████████████████████| 77.0/77.0 [00:00<00:00, 4.05kiB/s]\n",
      "encoder.json: 100%|███████████████████████| 1.04M/1.04M [00:00<00:00, 9.01MiB/s]\n",
      "hparams.json: 100%|█████████████████████████| 90.0/90.0 [00:00<00:00, 15.6kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|███████| 498M/498M [00:39<00:00, 12.5MiB/s]\n",
      "model.ckpt.index: 100%|███████████████████| 5.21k/5.21k [00:00<00:00, 1.85MiB/s]\n",
      "model.ckpt.meta: 100%|██████████████████████| 471k/471k [00:00<00:00, 7.23MiB/s]\n",
      "vocab.bpe: 100%|████████████████████████████| 456k/456k [00:00<00:00, 7.18MiB/s]\n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings:  {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embeddings weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings: \", settings)\n",
    "\n",
    "print(params[\"wte\"])\n",
    "print(\"Token embeddings weight tensor dimensions:\", params[\"wte\"].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer above loaded model to our GPTModel..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model config in a dictionary\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 26},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign OpenAI weights to corresponding weights of GPTModel instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].attn.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].attn.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].attn.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].attn.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].attn.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].attn.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].attn.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].attn.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].attn.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].attn.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].attn.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].attn.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].attn.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].attn.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].attn.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].attn.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "    \n",
    "    \n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lessr_venv",
   "language": "python",
   "name": "lessr_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
